{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* Adapted from COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll discuss:\n",
    "* support vector machines (SMV)\n",
    "* generalizability and cross validation\n",
    "* feature selection\n",
    "* comparing classification methods\n",
    "\n",
    "Recommended Reading: \n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 9 [digitial version available here](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "* A. GÃ©ron, Hands-On Machine Learning with Scikit-Learn & TensorFlow (2017), Ch. 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_moons, load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification \n",
    "\n",
    "Recall that in *classification* we attempt to predict a categorical variable based on based on several features or attributes. \n",
    "\n",
    "We've already seen three methods for classification: \n",
    "1. Logistic Regression\n",
    "+ k Nearest Neighbors (k-NN)\n",
    "+ Decision Trees\n",
    "\n",
    "Today, we'll see that [Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine) are another method for classification. These methods are especially good at classifying small to medium sized, complex datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "** Idea: ** Use tools from optimization to find the *best* lines (or hyperplanes or curves) that divide the two datasets. \n",
    "\n",
    "### Simplest version first: 2 classes and no \"kernel transformation\"\n",
    "\n",
    "Given $n$ samples $(\\vec{x}_1,y_1), (\\vec{x}_2,y_2),\\ldots,(\\vec{x}_n,y_n)$, where $x_i$ are attributes or features and $y_i$ are categorical variables that you want to predict. We assume that each $\\vec{x}_i$ is a real vector and the $y_i$ are either $1$ or $-1$ indicating the class. \n",
    "\n",
    "** Goal: **  Find the \"maximum-margin line\" or more generally, the \"maximum-margin hyperplane\" that divides the two classes, which is defined so that the distance between the hyperplane and the nearest point in either group is maximized. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png\" width=\"400\">\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad$ \n",
    "source: [wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "\n",
    "\n",
    "We can write any line as the set of points $\\vec{x}$ satisfying \n",
    "$$\\vec{w}\\cdot\\vec{x} = b$$ \n",
    "for some (normal) vector $\\vec{w}$ and number $b$. The goal is to determine $\\vec{w}$ and $b$. Once we have the best values of $\\vec{w}$ $b$, our classifier is simply\n",
    "$$\n",
    "\\vec{x} \\mapsto \\textrm{sgn}(\\vec{w} \\cdot \\vec{x} - b).\n",
    "$$\n",
    "\n",
    "** Hard-margin.** If the data is linearly separable, then we find the separating line (hyperplane) with the *largest margin* ($b/\\|w\\|$). \n",
    "\n",
    "This is given by the solution to the optimization problem \n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\vec{w},b}  & \\  \\|\\vec{w}\\|^2  \\\\ \n",
    "\\textrm{ subject to } & \\  y_i(\\vec{w}\\cdot\\vec{x}_i - b) \\ge 1 , \\textrm{for } i = 1,\\,\\ldots,\\,n \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "This can be written as a *convex optimization problem* and efficiently solved. Take Math 5770/6640: Introduction to Optimization to learn more! In this class, we'll simply use scikit-learn. \n",
    "\n",
    "From the picture, the max-margin hyperplane is determined by the $\\vec{x}_i$ which are closest to it. These $\\vec{x}_i$ are called *support vectors*.\n",
    "\n",
    "\n",
    "** Soft-margin.** For data that is not linearly separable (e.g., two moons dataset), we introduce a *penalty* or *loss* function for violating the constraint,  $y_i(\\vec{w}\\cdot\\vec{x}_i + b) \\ge 1$. One of these is the *hinge loss* function, given by \n",
    "$$\n",
    "g(x_i; \\vec{w},b) = \\max\\left(0, 1-y_i(\\vec{w}\\cdot\\vec{x}_i - b)\\right). \n",
    "$$\n",
    "We can see that this function is zero if the constraint is satisfied. If the constraint is not satisfied, we pay a penatly, which is proportional to the distance to the separating hyperplane.\n",
    "\n",
    "We then fix the parameter $C > 0$ and solve the problem \n",
    "$$\n",
    "\\min_{\\vec{w},b}  \\ \\frac{1}{n} \\sum_{i=1}^n g(x_i; \\vec{w},b)  \\ + \\ C \\|\\vec{w}\\|^2  . \n",
    "$$\n",
    "The parameter $C$ determines how much we penalize points for being on the wrong side of the line. \n",
    "\n",
    "**Question:** How to choose the parameter $C$? Cross Validation! (more on this below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM and two moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# there are two features contained in X and the labels are contained in y\n",
    "X,y = make_moons(n_samples=500,random_state=1,noise=0.3)\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# These two models give the same solution, but \n",
    "# the second method is faster for a 'linear' kernel\n",
    "\n",
    "#model = svm.SVC(kernel='linear', C=10)\n",
    "model = LinearSVC(C=100,loss=\"hinge\")\n",
    "\n",
    "model.fit(X, y)    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "# Plot the predictions made by SVM\n",
    "x_min, x_max = X[:,0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=ListedColormap(['DarkRed', 'DarkBlue']), alpha=.2)\n",
    "plt.contour(xx, yy, zz, colors=\"black\", alpha=1, linewidths=0.2) \n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Classification of Two Moons using SVM')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = model.predict(X)\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines \n",
    "\n",
    "** Basic idea: ** The dataset may not be linearly separable, but it may be if we apply a *nonlinear kernel transformation*. \n",
    "\n",
    "** Example: ** two \"rings\" of points\n",
    "\n",
    "** Hard part: ** Not increasing the computationally complexity of the optimization problem. \n",
    "\n",
    "\n",
    "** (Very) rough sketch: **\n",
    "Replace the linear classifier \n",
    "$$\n",
    "\\vec{x} \\mapsto \\textrm{sgn}(\\vec{w} \\cdot \\vec{x} - b).\n",
    "$$\n",
    "with a nonlinear one \n",
    "$$\n",
    "\\vec{x} \\mapsto \\textrm{sgn}( f(x)).\n",
    "$$\n",
    "\n",
    "Choices of the nonlinear transformation in scikit-learn are:\n",
    "- linear: $\\langle x, x'\\rangle$.\n",
    "- polynomial: $(\\gamma \\langle x, x'\\rangle + r)^d$. $d$ is specified by keyword degree, $r$ by coef0.\n",
    "- rbf: $\\exp(-\\gamma |x-x'|^2)$. $\\gamma$ is specified by keyword gamma, must be greater than 0.\n",
    "- sigmoid:  $\\tanh(\\gamma \\langle x,x'\\rangle + r)$, where $r$ is specified by coef0.\n",
    "\n",
    "The most popular choice is rbf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf', gamma='auto', C=50)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "# Plot the predictions made by SVM\n",
    "x_min, x_max = X[:,0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=ListedColormap(['DarkRed', 'DarkBlue']), alpha=.2)\n",
    "plt.contour(xx, yy, zz, colors=\"black\", alpha=1, linewidths=0.2) \n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Classification of Two Moons using SVM')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = model.predict(X)\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More than 2 classes?  \n",
    "There are a few different methods for extending binary classification to more than 2 classes. \n",
    "-  **one vs. one:** Consider all pairs of classes (if there are $k$ classes, then there are $\\binom k 2$ different pairs of classes. For each pair, we develop a classifier. For a new point, we perform all classifications and then choose the class that was most frequently assigned. (The classifiers all vote on a class.)\n",
    "\n",
    "- ** all vs. one:**  We compare one of the $k$ classes to the remaining $k-1$ classes. We assign the class to the observation which we are most confident (we have to make this precise).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Dataset: The Iris dataset\n",
    "\n",
    "The dataset contains 4 features (attributes) of 50 samples containing 3 different types of iris plants. The goal is to classify the type of iris plant given the attributes. \n",
    "\n",
    "**Features (attributes):**\n",
    "1. sepal length (cm) \n",
    "+ sepal width (cm) \n",
    "+ petal length (cm) \n",
    "+ petal width (cm) \n",
    "\n",
    "**Classes:**\n",
    "1. Iris Setosa \n",
    "+ Iris Versicolour \n",
    "+ Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "df = sns.load_dataset(\"iris\") # built-in dataset in seaborn \n",
    "sns.pairplot(df, hue=\"species\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# import data, scikit-learn also has this dataset built-in\n",
    "iris = load_iris()\n",
    "\n",
    "# For easy plotting and interpretation, we only use first 2 features here. \n",
    "# We're throwing away useful information - don't do this at home! \n",
    "X = iris.data[:,:2]\n",
    "y = iris.target\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y,  marker=\"o\", cmap=cmap_bold, s=30)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)    \n",
    "plt.title('Iris dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# SVM \n",
    "svm_iris = svm.SVC(kernel='rbf',gamma='scale', C=10)\n",
    "svm_iris.fit(X, y)\n",
    "\n",
    "# plot classification \n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = svm_iris.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, zz, cmap=cmap_light)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=30)\n",
    "\n",
    "plt.title('Classification of Iris dataset using SVM')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "y_pred = svm_iris.predict(X)\n",
    "print(metrics.confusion_matrix(y_true = y, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y, y_pred = y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Validation \n",
    "\n",
    "** Recall:** Cross validation is a method for assessing how well a model will generalize to an independent data set. \n",
    "\n",
    "** Idea:** We split the dataset into a *training set* and a *test set*. We train the model on the training set. We measure the accuracy of the model on the test set. \n",
    "\n",
    "There are several different ways to do this. Two popular methods are: \n",
    "- **k-fold cross validation** The data is randomly partitioned into k (approximately) equal sized subsamples (folds). For each of the k folds, the method is trained on the other k-1 folds and tested on that fold. The accuracy is computed using each of the k folds as the test set. \n",
    "- **leave-one-out cross validation** Same as above with k=n\n",
    "\n",
    "A variety of function are implemented in scikit-learn for cross validation. \n",
    "\n",
    "- The [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function randomly splits the data for cross validation\n",
    "- The [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function splits the data and measures accuracy on the test set.  The parameter *cv* is the k in k-fold cross validation. The parameter *scoring* specifies how you want to evaluate the model (we can just use accuracy). \n",
    "- The [cross_val_predict](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) function is similar to *cross_val_score*, but gives you the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# note: here we are again using all 4 features of the data\n",
    "scores = cross_val_score(estimator = svm_iris, X = iris.data, y = iris.target, cv=5, scoring='accuracy')\n",
    "print(scores)\n",
    "\n",
    "predicted = cross_val_predict(estimator = svm_iris, X = iris.data, y = iris.target)\n",
    "print(predicted)\n",
    "metrics.accuracy_score(iris.target, predicted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Incorporating cross-validation to choose SVM parameter, $C$\n",
    "\n",
    "Recall above that when the data is not linearly separable, introduce a loss function $g$ which penalizes the violation of datapoints lying on the wrong side of the hyperplane and solve the problem \n",
    "$$\n",
    "\\min_{\\vec{w},b}  \\ \\frac{1}{n} \\sum_{i=1}^n g(x_i; \\vec{w},b)  \\ + \\ C \\|\\vec{w}\\|  . \n",
    "$$\n",
    "Here, the parameter $C>0$ determines how much we penalize points for being on the wrong side of the line. \n",
    "\n",
    "Generally, the function *get_params()* an be used to see what parameters are available to change in a model. \n",
    "\n",
    "We can use cross-validation to choose the parameter $C$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "svm_iris.get_params()\n",
    "\n",
    "Cs = np.linspace(.01,200,100)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs): \n",
    "    svm_iris = svm.SVC(kernel='rbf',gamma='scale', C = C)\n",
    "    scores = cross_val_score(estimator = svm_iris, X = iris.data, y = iris.target, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "        \n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Based on the cross validation results, I would choose $C\\approx 3$. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
