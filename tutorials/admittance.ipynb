{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad School Admittance\n",
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from   random import random\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from   matplotlib.pyplot import plot\n",
    "from   matplotlib import pyplot as plt\n",
    "\n",
    "from   sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( \"admittance_train.csv\", skiprows=2 )\n",
    "df\n",
    "df.drop( columns=df.columns[0], inplace=True ) # Drop the original index column\n",
    "print( df.columns.values )\n",
    "print( \"GMAT: Min: %d, Max: %d, Avg: %d\" % ( df.gmat.min(), df.gmat.max(), df.gmat.mean() ) )\n",
    "print( \"GPA:  Min: %.1f, Max: %.1f, Avg: %.1f\" % ( df.gpa.min(),  df.gpa.max(),  df.gpa.mean() ) )\n",
    "print( \"Work: Min: %.1f, Max: %.1f, Avg: %.1f\" % ( df.work_experience.min(), df.work_experience.max(), df.work_experience.mean() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots( 1, 3 )\n",
    "\n",
    "fig.set_figheight( 2 )\n",
    "fig.set_figwidth(  14 )\n",
    "\n",
    "gpa  = axes[0]\n",
    "work = axes[1]\n",
    "gmat = axes[2]\n",
    "\n",
    "gpa.set_title( \"GPA\" )\n",
    "work.set_title( \"Work Exp\" )\n",
    "gmat.set_title( \"GMAT\" )\n",
    "\n",
    "axes[0].set_ylabel( \"Admitted\" )\n",
    "\n",
    "gpa.plot(  df.gpa,             df.admitted, '.' )\n",
    "work.plot( df.work_experience, df.admitted, '.' )\n",
    "gmat.plot( df.gmat,            df.admitted, '.' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_gpa_v_admitted  = smf.logit( \"admitted ~ gpa\", data=df ).fit( disp=0 ) # disp = 0 silences output\n",
    "r_gmat_v_admitted = smf.logit( \"admitted ~ gmat\", data=df ).fit( disp=0 )\n",
    "r_work_v_admitted = smf.logit( \"admitted ~ work_experience\", data=df ).fit( disp=0 )\n",
    "\n",
    "r_all_v_admitted  = smf.logit( \"admitted ~ gpa + gmat + work_experience\", data=df ).fit( disp=0 )\n",
    "\n",
    "print( \"Admit vs GPA:  %.2f%%\"   % ( r_gpa_v_admitted.prsquared * 100 ) )\n",
    "print( \"Admit vs GMAT: %.2f%%\"   % ( r_gmat_v_admitted.prsquared * 100 ) )\n",
    "print( \"Admit vs Work: %.2f%%\\n\" % ( r_work_v_admitted.prsquared * 100 ) )\n",
    "print( \"Admit vs All:  %.2f%%\" % ( r_all_v_admitted.prsquared * 100 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at how the training data lines up with the formula created from the training data... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how each of our training data points fits into the created formula.\n",
    "\n",
    "x_train = df[ [ \"gmat\", \"gpa\", \"work_experience\" ] ]\n",
    "df[ \"train_predictions\" ] = r_all_v_admitted.predict( x_train )\n",
    "\n",
    "# Plot predicted admit that were admitted\n",
    "df_correct = df[ (df.train_predictions >= 0.5) & (df.admitted == 1.0) ]\n",
    "plot( df_correct.train_predictions, 'go' )\n",
    "\n",
    "# Plot predicted non-admit that were not admitted\n",
    "df_correct = df[ (df.train_predictions < 0.5) & (df.admitted == 0.0) ]\n",
    "plot( df_correct.train_predictions, 'go' )\n",
    "\n",
    "# Plot predicted admit that were NOT admitted\n",
    "df_false_pos = df[ (df.train_predictions >= 0.5) & (df.admitted == 0.0) ]\n",
    "plot( df_false_pos.train_predictions, 'ro' )\n",
    "\n",
    "# Plot predicted NOT-admit that were admitted\n",
    "df_false_neg = df[ (df.train_predictions < 0.5) & (df.admitted == 1.0) ]\n",
    "plot( df_false_neg.train_predictions, 'yo' )\n",
    "\n",
    "print( \"False Negatives (We predict that this student would not be admitted):\" )\n",
    "print( df_false_neg )\n",
    "\n",
    "print( \"False Positives (We predicted that these students would be admitted):\" )\n",
    "print( df_false_pos )\n",
    "\n",
    "predictions = list( map( round, df.train_predictions.values ) )\n",
    "print(\"pre:\", predictions)\n",
    "\n",
    "cm = confusion_matrix( df.admitted, predictions )\n",
    "disp = ConfusionMatrixDisplay( cm, display_labels=[ \"Not Admitted\", \"Admitted\" ] )\n",
    "axes = disp.plot()\n",
    "axes.ax_.set( xlabel=\"Predicted Admit\", ylabel=\"Actual Admit\")\n",
    "\n",
    "print( \"Prediction accuracy:\", accuracy_score( df.admitted, predictions ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Prediction Formula Using Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv( \"admittance_test.csv\" )\n",
    "df_test.drop( columns=df_test.columns[0], inplace=True ) # Drop the original index column\n",
    "\n",
    "x_test = df_test[ [ \"gmat\", \"gpa\", \"work_experience\" ] ]\n",
    "\n",
    "yhat = r_all_v_admitted.predict( x_test )\n",
    "testing_predictions = list( map( round, yhat ) ) # Turn 0.0-1.0 percentages into discreet 0 or 1\n",
    "\n",
    "print( \"Actual Admittance:   \", list( df_test.admitted.values ) )\n",
    "print( \"Predicted Admittance:\", list( testing_predictions ) )\n",
    "\n",
    "cm = confusion_matrix( df_test.admitted, testing_predictions )\n",
    "disp = ConfusionMatrixDisplay( cm, display_labels=[ \"Not Admitted\", \"Admitted\" ] )\n",
    "axes = disp.plot()\n",
    "axes.ax_.set( xlabel=\"Predicted Admit\", ylabel=\"Actual Admit\")\n",
    "\n",
    "print( \"Prediction accuracy: %.1f%%\" % ( accuracy_score( df_test.admitted, testing_predictions ) * 100 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df[ [ \"gmat\", \"gpa\", \"work_experience\" ] ]\n",
    "yhat_training = r_all_v_admitted.predict( x_train )\n",
    "\n",
    "params = dict( r_all_v_admitted.params )\n",
    "\n",
    "########################################################\n",
    "# Training Data:\n",
    "\n",
    "scores = []\n",
    "for gmat, gpa, work, _, _ in df.values:\n",
    "    score = params[ \"Intercept\" ] + gpa * params[ \"gpa\" ] + gmat * params[ \"gmat\" ] + work * params[ \"work_experience\" ]\n",
    "    scores.append( score )\n",
    "\n",
    "#plot( scores, testing_predictions, '.' )\n",
    "plot( scores, yhat_training, '.' )\n",
    "\n",
    "########################################################\n",
    "# Testing data:\n",
    "\n",
    "scores = []\n",
    "for gmat, gpa, work, _ in df_test.values:\n",
    "    score = params[ \"Intercept\" ] + gpa * params[ \"gpa\" ] + gmat * params[ \"gmat\" ] + work * params[ \"work_experience\" ]\n",
    "    scores.append( score )\n",
    "\n",
    "plot( scores, yhat, 'ro' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of (Fabricated) Exact Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we are going to (pretend) that students were admitted based only on\n",
    "# having a 3.5 or higher GPA.  Note: Noise must be added to make regression converge.\n",
    "\n",
    "min_gpa = 3.5\n",
    "\n",
    "def noise( x ):\n",
    "    mod = (random() - 0.5) / 16.0 # Little noise / uncertainty\n",
    "    #mod = (random() - 0.5) # Lots of noice and uncertainty\n",
    "    return ( x + mod ) >= min_gpa\n",
    "\n",
    "df = pd.read_csv( \"admittance-400.csv\" )\n",
    "\n",
    "df[\"Admitted_On_GPA\"] = df.gpa.apply( noise ).astype( float )\n",
    "print( \"# admitted, # denied:\", len( df[ df.Admitted_On_GPA == 1.0 ] ), len( df[ df.Admitted_On_GPA == 0.0 ] ) )\n",
    "\n",
    "print( \"Low GPAs Admitted:\" )\n",
    "print( len( df[ (df.Admitted_On_GPA==1.0) & ( df.gpa < min_gpa ) ] ) )\n",
    "# print( df[ (df.Admitted_On_GPA==1.0) & ( df.gpa < min_gpa ) ] )\n",
    "print( \"High GPAs NOT Admitted:\" )\n",
    "print( len( df[ (df.Admitted_On_GPA==0.0) & ( df.gpa >= min_gpa ) ] ) )\n",
    "# print( df[ (df.Admitted_On_GPA==0.0) & ( df.gpa >= min_gpa ) ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = lambda x: 1 / ( 1 + np.exp(-x) )\n",
    "\n",
    "odds  = lambda p: p/(1-p)\n",
    "logit = lambda p: np.log( odds(p) )\n",
    "\n",
    "r0  = smf.logit( \"Admitted_On_GPA ~ gpa\", data=df ).fit( disp=0 ) # disp = 0 silences output\n",
    "print( \"~r^2: %.3f%%\" % ( r0.prsquared * 100 ) )\n",
    "\n",
    "plt.figure( figsize=( 15, 3) )\n",
    "plot( df.gpa, df.Admitted_On_GPA, \".\" )\n",
    "plot( [min_gpa,min_gpa], [0,1], 'r')\n",
    "\n",
    "par = dict( r0.params )\n",
    "xs = np.linspace( df.gpa.min(), df.gpa.max(), 1000 )\n",
    "ys = logistic( par['Intercept'] + par['gpa']*xs )\n",
    "plt.plot( xs, ys, color='Black' ) # Smushed line\n",
    "\n",
    "plt.xlabel( \"GPA\" )\n",
    "plt.ylabel( \"Admitted\" )\n",
    "plt.title( \"Predicted Admittance vs GPA\" )\n",
    "#plt.xlim( [3.4, 3.6] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of likelihood depends slope of line...\n",
    "prob_admit = 0.9\n",
    "print( \"GPA needed (%.0f%% likely): %.2f\" % ( prob_admit*100, ( logit(prob_admit) - par['Intercept'] ) /par['gpa'] ) )\n",
    "prob_admit = 0.75\n",
    "print( \"GPA needed (%.0f%% likely): %.2f\" % ( prob_admit*100, ( logit(prob_admit) - par['Intercept'] ) /par['gpa'] ) )\n",
    "prob_admit = 0.5\n",
    "print( \"GPA needed (%.0f%% likely): %.2f\" % ( prob_admit*100, ( logit(prob_admit) - par['Intercept'] ) /par['gpa'] ) )\n",
    "prob_admit = 0.10\n",
    "print( \"GPA needed (%.0f%% likely): %.2f\" % ( prob_admit*100, ( logit(prob_admit) - par['Intercept'] ) /par['gpa'] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regress using both GPA and GRE - NOTE: Only GPA was used to determine admittance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1  = smf.logit( \"Admitted_On_GPA ~ gpa + gre\", data=df ).fit( disp=0 ) # disp = 0 silences output\n",
    "\n",
    "print( \"~r^2: %.3f%%, p-values: gpa: %.3f, gre: %.3f\" % ( ( r1.prsquared * 100 ), r1.pvalues[\"gpa\"], r1.pvalues[\"gre\"] ) )\n",
    "\n",
    "admitted     = df[ df.Admitted_On_GPA == 1.0 ]\n",
    "not_admitted = df[ df.Admitted_On_GPA == 0.0 ]\n",
    "# print(\"len:\", len( not_admitted ), len( admitted ) )\n",
    "\n",
    "plot( admitted.gpa, admitted.gre, 'g.' )\n",
    "plot( not_admitted.gpa, not_admitted.gre, 'r.' )\n",
    "\n",
    "xs = np.linspace( 3.0, 4.0, 1000 )\n",
    "par = dict( r1.params )\n",
    "# solve 0.5 = logistic(b0 + b1*xs + b2*y) for y\n",
    "# logit(0.5) = b0 + b1*xs + b2*y\n",
    "# y = (logit(.5) - b0 - b1*x)/b2\n",
    "plt.plot( xs, (1/par['gre'])*(logit(.5) - par['Intercept'] - par['gpa']*xs), color='Black' )\n",
    "plt.ylim( [200, 850] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admit using GRE and GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_gpa( x ) :\n",
    "    return x + ( random() - 0.5 ) / 16\n",
    "\n",
    "def noise_gre( x ) :\n",
    "    return x + ( random() - 0.5 ) * 100\n",
    "\n",
    "rgpa = df.gpa.apply( noise_gpa ) / 4.0\n",
    "rgre = df.gre.apply( noise_gre ) / 800\n",
    "#rgpa.sort_values()\n",
    "\n",
    "#(( rgre + rgpa ) / 2).sort_values()\n",
    "df[ \"Admitted_on_GPA_GRE\" ] = ( ( ( 10 * rgre + rgpa ) / 11) > .9 ).astype( float )\n",
    "\n",
    "print( len( df[ df.Admitted_on_GPA_GRE == 1.0 ] ), len( df[ df.Admitted_on_GPA_GRE == 0.0 ] ) )\n",
    "\n",
    "r1  = smf.logit( \"Admitted_on_GPA_GRE ~ gpa + gre\", data=df ).fit( disp=0 ) # disp = 0 silences output\n",
    "r1.summary()\n",
    "\n",
    "admitted     = df[ df.Admitted_on_GPA_GRE == 1.0 ]\n",
    "not_admitted = df[ df.Admitted_on_GPA_GRE == 0.0 ]\n",
    "# print(\"len:\", len( not_admitted ), len( admitted ) )\n",
    "\n",
    "\n",
    "plot( admitted.gpa, admitted.gre, 'g.' )\n",
    "plot( not_admitted.gpa, not_admitted.gre, 'r.' )\n",
    "\n",
    "xs = np.linspace( 2.0, 5.0, 1000 )\n",
    "par = dict( r1.params )\n",
    "plt.plot( xs, (1/par['gre'])*(logit(.5) - par['Intercept'] - par['gpa']*xs), color='Black' )\n",
    "plt.xlim( [2.0, 4.1] )\n",
    "plt.ylim( [200, 850] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model uses GRE and GPA, but actual ranking also used undergrad school's rank to factor GPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"rank\" to \"prestige\" as \"rank\" is a reserved field\n",
    "df.rename( columns={ \"rank\": \"prestige\"}, inplace=True )\n",
    "\n",
    "# Original prestige of 1 is the best... but that makes it hard to use as a scalar...\n",
    "# So flip the values... now 4 == best, 1 == worse\n",
    "df['adjusted_prestige'] = 5 - df['prestige']\n",
    "\n",
    "# This leaves: rank of 4 (best) == 100%... Rank of 1 (worse) == 87%\n",
    "# agpa = (((df.adjusted_prestige+1000)/1004)*rgpa).sort_values()\n",
    "agpa = (((df.adjusted_prestige+20)/24)*rgpa).sort_values()\n",
    "\n",
    "df[ \"Admitted_on_GPA_GRE_Rank\" ] = ( ( ( rgre + agpa ) / 2) > .80 ).astype( float )\n",
    "\n",
    "print( len( df[ df.Admitted_on_GPA_GRE_Rank == 1.0 ] ), len( df[ df.Admitted_on_GPA_GRE_Rank == 0.0 ] ) )\n",
    "\n",
    "# r1  = smf.logit( \"Admitted_on_GPA_GRE_Rank ~ gre + gpa\", data=df ).fit( disp=0 ) # disp = 0 silences output\n",
    "r1  = smf.logit( \"Admitted_on_GPA_GRE_Rank ~ gre + gpa + prestige\", data=df ).fit( disp=0 ) # disp = 0 silences output\n",
    "print( r1.summary() )\n",
    "\n",
    "admitted     = df[ df.Admitted_on_GPA_GRE_Rank == 1.0 ]\n",
    "not_admitted = df[ df.Admitted_on_GPA_GRE_Rank == 0.0 ]\n",
    "print(\"len:\", len( not_admitted ), len( admitted ) )\n",
    "\n",
    "plot( admitted.gpa, admitted.gre, 'g.' )\n",
    "plot( not_admitted.gpa, not_admitted.gre, 'r.' )\n",
    "\n",
    "xs = np.linspace( 2.0, 5.0, 1000 )\n",
    "par = dict( r1.params )\n",
    "plt.plot( xs, (1/par['gre'])*(logit(.5) - par['Intercept'] - par['gpa']*xs), color='Black' )\n",
    "plt.xlim( [2.0, 4.1] )\n",
    "plt.ylim( [200, 850] )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
