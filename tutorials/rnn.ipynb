{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Here we'll play with a hand-rolled RNN that works on text data and see what we can do with it.\n",
    "\n",
    "This comes from Andrej Karpathy (currently director of AI at Tesla, and my old labmate who I'm coauthor on a paper with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BasicRNN:\n",
    "    def __init__(self, data, hidden_size=128, seq_length=64,learning_rate=1e-1):\n",
    "        self.data = data;\n",
    "        self.chars = list(set(data))\n",
    "        self.data_size, self.vocab_size = len(data), len(self.chars)\n",
    "        print ('data has %d characters, %d unique.' % (self.data_size, self.vocab_size))\n",
    "        \n",
    "        # what is our \"alphabet\" ?  We're going to number the characters from \n",
    "        # 0 to numUniqueCharacters\n",
    "        # and store forward and reverse mappings \n",
    "        # (what index is this char, what char is at this index?)\n",
    "        self.char_to_ix = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.ix_to_char = { i:ch for i,ch in enumerate(self.chars) }\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hidden_size = hidden_size # size of hidden layer of neurons\n",
    "        self.seq_length = seq_length # number of steps to unroll the RNN for\n",
    "        self.learning_rate = learning_rate #for optimization\n",
    "\n",
    "        # model parameters\n",
    "        # these are the weights that we're training\n",
    "        self.Wxh = np.random.randn(self.hidden_size, self.vocab_size)*0.01 # input to hidden\n",
    "        self.Whh = np.random.randn(self.hidden_size, self.hidden_size)*0.01 # hidden to hidden\n",
    "        self.Why = np.random.randn(self.vocab_size, self.hidden_size)*0.01 # hidden to output\n",
    "        self.bh = np.zeros((self.hidden_size, 1)) # hidden bias\n",
    "        self.by = np.zeros((self.vocab_size, 1)) # output bias\n",
    "\n",
    "    # measures the error (how far arr of ar the targets from what the model predicts)\n",
    "    # and the gradients (the calculus used to improve the weights)\n",
    "    def lossFun(self,inputs, targets, hprev):\n",
    "      \"\"\"\n",
    "      inputs,targets are both list of integers.\n",
    "      hprev is Hx1 array of initial hidden state\n",
    "      returns the loss, gradients on model parameters, and last hidden state\n",
    "      \"\"\"\n",
    "      xs, hs, ys, ps = {}, {}, {}, {}\n",
    "      hs[-1] = np.copy(hprev)\n",
    "      loss = 0\n",
    "      # forward pass.  This computes the actual error\n",
    "      for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh) # hidden state\n",
    "        ys[t] = np.dot(self.Why, hs[t]) + self.by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars (apply softmax)\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "      \n",
    "    \n",
    "      # backward pass: compute gradients going backwards\n",
    "      # this is the calculus part\n",
    "      dWxh, dWhh, dWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "      dbh, dby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "      dhnext = np.zeros_like(hs[0])\n",
    "      for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(self.Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(self.Whh.T, dhraw)\n",
    "      for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "      return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "    def sample(self, h, seed_ix, n):\n",
    "      \"\"\" \n",
    "      sample a sequence of integers from the model \n",
    "      h is memory state, seed_ix is seed letter for first time step\n",
    "      \"\"\"\n",
    "      x = np.zeros((self.vocab_size, 1))\n",
    "      x[seed_ix] = 1\n",
    "      ixes = []\n",
    "      for t in range(n):\n",
    "        #set up the hidden state properly\n",
    "        h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "        y = np.dot(self.Why, h) + self.by\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) #run the input through the network\n",
    "        #pick the next character based on the probabilities output by the network\n",
    "        ix = np.random.choice(range(self.vocab_size), p=p.ravel()) \n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "      return ixes\n",
    "\n",
    "    # perform the optimization to find the best weights\n",
    "    def train(self, nIters = 10000):\n",
    "        n, p = 0, 0\n",
    "        mWxh, mWhh, mWhy = np.zeros_like(self.Wxh), np.zeros_like(self.Whh), np.zeros_like(self.Why)\n",
    "        mbh, mby = np.zeros_like(self.bh), np.zeros_like(self.by) # memory variables for Adagrad\n",
    "        smooth_loss = -np.log(1.0/self.vocab_size)*self.seq_length # loss at iteration 0\n",
    "        while n < nIters:\n",
    "          # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "          if p+self.seq_length+1 >= len(self.data) or n == 0: \n",
    "            hprev = np.zeros((self.hidden_size,1)) # reset RNN memory\n",
    "            p = 0 # go from start of data\n",
    "          inputs = [self.char_to_ix[ch] for ch in self.data[p:p+self.seq_length]]\n",
    "          targets = [self.char_to_ix[ch] for ch in self.data[p+1:p+self.seq_length+1]]\n",
    "\n",
    "          # sample from the model now and then\n",
    "          if n % 200 == 0:\n",
    "            sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "            txt = ''.join(self.ix_to_char[ix] for ix in sample_ix)\n",
    "            print( '----\\n %s \\n----' % (txt, ))\n",
    "          #print(\"inp: \", inputs, \"targets: \", targets, \"hprev: \", hprev)\n",
    "          # forward seq_length characters through the net and fetch gradient\n",
    "          loss, dWxh, dWhh, dWhy, dbh, dby, hprev = self.lossFun(inputs, targets, hprev)\n",
    "          smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "          if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss) )# print progress\n",
    "  \n",
    "          # perform parameter update with Adagrad\n",
    "          for param, dparam, mem in zip([self.Wxh, self.Whh, self.Why, self.bh, self.by], \n",
    "                                        [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                        [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "          p += self.seq_length # move data pointer\n",
    "          n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.0/7.0\n",
    "s = str(x)\n",
    "\n",
    "pattern = s[2:8]\n",
    "pattern *= 1000\n",
    "\n",
    "print( s )\n",
    "print( pattern[:30] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatingRNN = BasicRNN( pattern )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatingRNN.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizzbuzz( N ):\n",
    "    out = \"\"\n",
    "    for i in range( 1, N+1 ):\n",
    "        if i % 3 == 0:\n",
    "            out += \"fizz\"\n",
    "            if i % 5 == 0:\n",
    "                out += \"buzz\"\n",
    "        elif i % 5 == 0:\n",
    "            out += \"buzz\"\n",
    "        else:\n",
    "            out += str( i )\n",
    "        out += \" \"\n",
    "    return out\n",
    "\n",
    "pattern = fizzbuzz( 100 )\n",
    "display( pattern )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbRNN = BasicRNN( fizzbuzz(100) )\n",
    "fbRNN.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "aiw = urllib.request.urlopen( \"http://www.gutenberg.org/cache/epub/19033/pg19033.txt\" )\n",
    "#dir(aiw)\n",
    "aiw = str( aiw.read(), 'utf=8' )\n",
    "aiw[ :100 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiwRNN = BasicRNN( aiw, hidden_size=512,seq_length=256 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiwRNN.train( 20000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiwRNN.train( 1000 )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
