{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science \n",
    "# Lecture 13: Classification 1\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecutre, we'll discuss:\n",
    "* a general overview of classification \n",
    "* logistic regression \n",
    "* k nearest neighbors (k-NN) \n",
    "* generalizability and cross validation \n",
    "\n",
    "Recommended Reading: \n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 4 [digitial version available here](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "* J. Grus, Data Science from Scratch, Ch.12,16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification \n",
    "\n",
    "Recall that in **regression**, we try to predict a real-valued (quantitative) variable. Examples:\n",
    "1. Predict house prices based on attributes\n",
    "+ predict credit score rating based on income, balance, gender, education, etc...\n",
    "\n",
    "\n",
    "In ** classification**, we try to predict a categorical (qualitative) variable. Examples:\n",
    "1. Predict whether a bank should issue a person a credit card (yes/no)\n",
    "+ Predict a hospital patient's diagnosis (stroke, heart attack,...) based on symptoms. \n",
    "\n",
    "**Setup:** Given $n$ samples $(x_1,y_1), (x_2,y_2),\\ldots,(x_n,y_n)$, where $x_i$ are attributes or features and $y_i$ are categorical variables that you want to predict. \n",
    "\n",
    "**Goal:** Develop a rule for predicting the categorical variable $y$ based on the features $x$. \n",
    "\n",
    "**Example:** Can the number of hours a student studies predict whether they will pass the exam?\n",
    "\n",
    "**Example:** Can we predict whether or not a student will be admitted to a graduate program based on their undergraduate performance (GPA, GRE score, prestige of student's undergraduate university)? \n",
    "\n",
    "**Example:** The post office uses classification to automatically sort mail by zip code. The digits of the zip code are assigned to one of the *classes:* $0,1,2,\\ldots,9$. \n",
    "\n",
    "We'll cover several classification methods:\n",
    "\n",
    "* logistic regression\n",
    "+ k nearest neighbors\n",
    "+ trees and random forests\n",
    "+ support vector machines\n",
    "* Neural Networks and Deep Learning\n",
    "\n",
    "**Evaluation:** How can we evaluate how well a classification algorithm has done? One way is via the confusion matrix. The *confusion matrix* is just a table where each column of the matrix represents the number of samples in each predicted class and each row represents the number of samples in each actual class. \n",
    "\n",
    "Consider the results from a classifier that is trying to classify 27 images of cats, dogs, and rabbits.\n",
    "![confusion matrix](./confusionMatrix.png)\n",
    "\n",
    "This classifier is very good at distinguishing between cats and rabbits but lousy at recognizing dogs...half are misclassified!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic regression \n",
    "\n",
    "In logistic regression, the categorical value, $Y$, that we predict only takes two values. We'll call them 0 and 1. \n",
    "\n",
    "**Main idea:** Given the predictor variable  X, we model the *probability* that $Y = 1$ by \n",
    "$$\n",
    "p(X) = p(Y = 1 | \\ X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1+ e^{\\beta_0 + \\beta_1 X}}\n",
    "$$\n",
    "\n",
    "Defining the *logistic function* or *sigmoid function*,  \n",
    "$$\n",
    "\\textrm{logistic}(x) := \\frac{e^x}{1 + e^x} = \\frac{1}{1+ e^{-x}},\n",
    "$$\n",
    "we can write the above expression more succinctly as \n",
    "$$\n",
    "p(X) = \\textrm{logistic}( \\beta_0 + \\beta_1 X ).  \n",
    "$$\n",
    "\n",
    "One can check that the model is equivlant to \n",
    "$$\n",
    "\\log \\left( \\frac{p(X)}{1-p(X)} \\right) = \\beta_0 + \\beta_1 X. \n",
    "$$\n",
    "\n",
    "Sometimes, the function on the left hand side of the above equation is called the *logit function*, \n",
    "$$\n",
    "\\textrm{logit}(p) := \\log \\left( \\frac{p}{1-p} \\right). \n",
    "$$\n",
    "We can equivalently write the model as \n",
    "\n",
    "$$\n",
    "\\textrm{logit} ( p(X) ) = \\beta_0 + \\beta_1 X.  \n",
    "$$\n",
    "It follows that the logistic and logit functions are inverses of each other. \n",
    "\n",
    "We interpret $\\frac{p(X)}{1-p(X)}$ as being the *odds* that $Y=1$. Note that $p(X)$ is a number between 0 and 1 so that the odds that $Y=1$ is a number between 0 and $\\infty$. If $p(X) = 0.5$, then odds = 1.  We also have that $\\textrm{logit} ( p(X) )$ is a number between $-\\infty$ and $\\infty$, so it makes sense that it could be linear in the data (as modeled). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "logistic = lambda x: 1 / ( 1 + np.exp(-x) )\n",
    "\n",
    "xs = np.linspace(-10,10,1000)\n",
    "ys = logistic( xs )\n",
    "plt.plot( xs, ys, color='black',label='logistic(x)' )\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('x')\n",
    "plt.ylim(-.1,1.1)\n",
    "plt.show()\n",
    "\n",
    "##################\n",
    "\n",
    "odds  = lambda p: p/(1-p)\n",
    "logit = lambda p: np.log( odds(p) )\n",
    "\n",
    "ps = np.linspace( 1e-5, 1-1e-5, 1000 )\n",
    "\n",
    "plt.plot( ps, odds(ps), color='black', label='odds(ps)' )\n",
    "plt.plot( ps, logit(ps), color='red', label='logit(ps)' )\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('p')\n",
    "plt.ylim(-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Logistic regression:** Given samples $(x_i,y_i)$ for $i=1,\\ldots,n$, find the *best* values of $\\beta_0$ and $\\beta_1$ so that \n",
    "$$\n",
    "y = \\textrm{logistic}( \\beta_0 + \\beta_1 X ) \n",
    "\\qquad \\textrm{or} \\qquad\n",
    "\\textrm{logit} ( y ) = \\beta_0 + \\beta_1 X.  \n",
    "$$\n",
    "Recall that in linear regression, we found the the coefficients $\\beta_0$ and $\\beta_1$ by setting up the sum of the squared residuals $\\sum_{i} (y_i - \\beta_0 - \\beta_1 x_i)^2$, setting the partial derivatives with respect to  $\\beta_0$ and $\\beta_1$ to zero, and solving for  $\\hat \\beta_0$ and $\\hat \\beta_1$. \n",
    "We can try to do the same thing here, but unlike linear regression, there is no longer a closed form solution for the best coefficients $\\hat \\beta_0$ and $\\hat \\beta_1$. We have to use a computer to find them. \n",
    "\n",
    "**Statistical inference viewpoint:** \n",
    "Above, logistic regression was described from the explanatory viewpoint of finding the best parameters to describe the data. \n",
    "Just like as in regression, there is also a statistical inference viewpoint. The categorical variable is modeled as \n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1 &  \\beta_0 + \\beta_1 x + \\epsilon > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "where $\\epsilon$ is a random variable that is distributed according to the logistic distribution. Here, we seek to infer the values of $\\beta_0$ and $\\beta_1$ from noisy samples of this variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression Example:  How many hours do you need to study to pass the exam? \n",
    "\n",
    "This example was taken from [wikipedia](https://en.wikipedia.org/wiki/Logistic_regression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hours = [0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25, 2.50, 2.75, 3.00, 3.25, 3.50, 4.00, 4.25, 4.50, 4.75, 5.00, 5.50] \n",
    "pass_Exam = [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
    "df = pd.DataFrame( {\"hours\": hours, \"pass_exam\": pass_Exam} )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.plot.scatter( x='hours', y='pass_exam', s=50 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For this example, we want to find coefficients $\\beta_0$ and $\\beta_1$ so that the model\n",
    "$$\n",
    "p(X) = \\textrm{probability}(\\textrm{passing} | \\ \\textrm{X hours studying} ) = \\textrm{logistic}( \\beta_0 + \\beta_1 X )\n",
    "$$\n",
    "best explains the data. \n",
    "\n",
    "We can use the 'logit' function from the statsmodels python library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "exam_model = sm.logit(formula=\"pass_exam ~ hours\", data=df).fit()\n",
    "exam_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print( df.columns )\n",
    "\n",
    "df.plot.scatter( x='hours', y='pass_exam', s=50 )\n",
    "\n",
    "par = dict( exam_model.params )\n",
    "\n",
    "xs = np.linspace( df['hours'].min(), df['hours'].max(), 1000 )\n",
    "ys = logistic( par['Intercept'] + par['hours']*xs )\n",
    "plt.plot( xs, ys, color='Black' ) # Smushed line\n",
    "\n",
    "############################\n",
    "# What does the Least squared regression line look like...\n",
    "#   - Look at the x-intercept on this (red) line.\n",
    "#\n",
    "# plt.plot( xs, par['Intercept'] + par['hours'] * xs, color=\"red\" ) # un-smushed version - regular regression\n",
    "# plt.ylim( [ -.1, 1.1 ] )\n",
    "\n",
    "############################\n",
    "# Where is the 50/50 point?  The location we will use to decide pass / fail? \n",
    "#   - (Also see confusion matrix below.)\n",
    "#\n",
    "# prob_passing = 0.50\n",
    "# hours_need_to_pass = logit( prob_passing ) - par['Intercept'] / par['hours'] \n",
    "\n",
    "# plt.plot( [hours_need_to_pass, hours_need_to_pass], [-1, 2], 'g' )\n",
    "# print(\"need:\", hours_need_to_pass)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The plot shows hours vs. whether or not the student passed and the logistic regression curve. The curve predicts the probability that a student will pass, given how many hours they've studied. \n",
    "\n",
    "From the plot, we can see that the more hours you spend studying, the more likely you are to pass the exam. *Let this be a lesson!* \n",
    "\n",
    "If I study just 2 hours, what is the likelihood that I'll pass? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print( \"2 hours of studying yields: %.2f%%\" % ( logistic(par['Intercept'] + par['hours']*2 ) * 100 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "I can also get this from exam_model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "exam_model.predict( pd.DataFrame( {\"hours\": [2, 3, 4, 5, 6]}, index=[2, 3, 4, 5, 6] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After studying just 2 hours, I'm only $26\\%$ likely to pass. \n",
    "\n",
    "How many hours do you have to study in order for the probability of passing to be greater than $50\\%$?\n",
    "\n",
    "From the plot, it looks like $\\approx 2.75$ hours. How can we find this number from the logistic regression? \n",
    "\n",
    "Recall: \n",
    "$$\n",
    "p(X) = \\textrm{logistic}( \\beta_0 + \\beta_1 X )\n",
    "$$\n",
    "\n",
    "Note: The $logit()$ function is the inverse of the $logistic()$ function.  p(X) is the probability of passing in this case if you study X hours.  So solving for X:\n",
    "$$\n",
    "logit( prob\\_pass ) = \\beta_0 + \\beta_1 X \\\\\n",
    "X = ( logit( prob\\_pass ) - \\beta_0 ) / \\beta_1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prob_passing = 0.75\n",
    "print( \"Hours needed: %.2f\" % ( ( logit(prob_passing) - par['Intercept'] ) /par['hours'] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can also make a confusion matrix for the model.  The confusion matrix shows us the number of data points we correctly classified (either as passing or failing), and the number of data points our model got wrong..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df['pred_pass'] = exam_model.predict() > .5\n",
    "pd.crosstab( index=df[\"pass_exam\"], columns=df[\"pred_pass\"] ) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We interpret this as: for each category (pass,fail), there were two mistakenly classified points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression Example: Graduate school admission based on undergraduate performance \n",
    "\n",
    "This example is based on [this blog post](http://blog.yhat.com/posts/logistic-regression-python-rodeo.html). \n",
    "\n",
    "\n",
    "**Dataset:**: For 400 students, we have the students attributes: GRE score, GPA, and the \"prestige\" of the student's undergraduate university (scale of 1 to 4, 1 best). We also have the variable that we want to predict: will they be accepted to grad school (yes/no)? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"admittance.csv\")\n",
    "\n",
    "# rename the 'rank' column because there is also a DataFrame method called 'rank'\n",
    "df.columns = [\"admit\", \"gre\", \"gpa\", \"prestige\"]\n",
    "\n",
    "#there are 400 students with 4 attributes: admit, gre, gpa, prestige\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_ = pd.plotting.scatter_matrix( df, figsize=(12, 12), diagonal='hist' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### A first attempt at logistic regression\n",
    "Let's first just focus on the GRE score. Our model is \n",
    "$$\n",
    "\\textrm{logit} ( \\textrm{probability admittance} ) = \\beta_0 + \\beta_1 * \\textrm{GRE}.  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "grad_model = sm.logit( formula=\"admit ~ gre\", data=df ).fit()\n",
    "grad_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.plot.scatter( x='gre', y='admit' )\n",
    "\n",
    "xs = np.linspace( df['gre'].min(), df['gre'].max(), 1000 )\n",
    "par = dict( grad_model.params )\n",
    "_ = plt.plot( xs, logistic( par['Intercept'] + par['gre']*xs) , color='Black' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[ 'pred_admit_1' ] = grad_model.predict() > .5 \n",
    "display( pd.crosstab( index=df[\"admit\"], columns=df[ \"pred_admit_1\" ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "With only the GRE score, we basically just predict that everyone's probability of being admitted to graduate school $< 50\\%$.  This is reflected in the confusion matrix. \n",
    "\n",
    "We need to include more data into our model! \n",
    "\n",
    "### A second attempt at logistic regression\n",
    "\n",
    "Include both GPA and GRE scores. Our model is \n",
    "$$\n",
    "\\textrm{logit} ( \\textrm{probability admittance} ) = \\beta_0 + \\beta_1 * \\textrm{GRE} + \\beta_2 * \\textrm{GPA} .  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "grad_model = sm.logit( formula=\"admit ~ gre + gpa\", data=df ).fit()\n",
    "grad_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[ 'pred_admit_2' ] = grad_model.predict() > .5\n",
    "print( \"Pandas Confusion Matrix:\" )\n",
    "print( pd.crosstab( index=df[\"admit\"], columns=df[\"pred_admit_2\"] ) ) # confusion matrix \n",
    "\n",
    "# print( \"SkLearn Confusion Matrix:\" )\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix( df.admit, df.pred_admit_2 )\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa_model = sm.logit( data=df, formula=\"admit ~ gpa\" ).fit()\n",
    "gpa_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='gpa',y='admit')\n",
    "\n",
    "x = np.linspace(df['gpa'].min(),df['gpa'].max(),1000)\n",
    "par = dict(gpa_model.params)\n",
    "plt.plot(x,logistic(par['Intercept'] + par['gpa']*x),color='Black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_admit_3'] = gpa_model.predict()>.5\n",
    "pd.crosstab(index=df[\"admit\"], columns=df[\"pred_admit_3\"]) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We're still predicting that most students are not being admitted - still not a very good model. This isn't so surprising if we look at a plot of GPA vs. GRE and try to see which students are admitted. You can't tell with your eye, so it's not surprising that the model fails too. (This is generally a good check.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ax = df[ df['admit'] == 0 ].plot.scatter( x='gpa',y='gre',color='red',label='not admitted' )\n",
    "df[ df['admit'] == 1 ].plot.scatter(x='gpa',y='gre',color='blue',label='admitted', ax=ax )\n",
    "\n",
    "xs = np.linspace( 3.5, 4.2, 1000 )\n",
    "par = dict( grad_model.params )\n",
    "# solve 0.5 = logistic(b0 + b1*xs + b2*y) for y\n",
    "# logit(0.5) = b0 + b1*xs + b2*y\n",
    "# y = (logit(.5) - b0 - b1*x)/b2\n",
    "plt.plot( xs, (1/par['gre'])*(logit(.5) - par['Intercept'] - par['gpa']*xs), color='Black' )\n",
    "\n",
    "plt.xlabel('GPA')\n",
    "plt.ylabel('GRE')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### A third attempt\n",
    "\n",
    "Finally, we include the 'prestige' of the school as a predictor variable.\n",
    "\n",
    "To get an idea of how many students are admitted from schools of different prestige, we can use the *crosstab* function in the pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab( df['admit'], df['prestige'], rownames=['admit'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We see that a smaller percentage of students are admitted from less prestigious universities. \n",
    "\n",
    "Recall that in linear regression, you can regress on ordinal variables by creating *dummy* variables (except baseline variable). We did this for the ethnicity variable in the credit dataset. One can similarly consider the prestige variable to be an ordinal variable and create a dummy variables...this is done [here](http://blog.yhat.com/posts/logistic-regression-python-rodeo.html) if you want to read about it. \n",
    "\n",
    "I found that the results are not much worse (and simpler) if we consider prestige to be an interval variable (equal increments between prestige levels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "grad_model = sm.logit( formula=\"admit ~ gre + gpa + prestige\", data=df ).fit()\n",
    "grad_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[ 'pred_admit_3' ] = grad_model.predict() > .5\n",
    "pd.crosstab( index=df[\"admit\"], columns=df[\"pred_admit_3\"] ) # confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We're still now correctly predicting ($253/273=92\\%$)  of the students who are not admitted. However, we're only correctly predicting ($29/127 = 23\\%$) of the students who are admitted. \n",
    "\n",
    "**Moral:** GRE, gpa, and 'prestige' alone do not predict admittance to graduate school. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['adjusted_prestige'] = 5 - df['prestige']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_model = sm.logit( formula=\"admit ~ gre + gpa + prestige + gpa*adjusted_prestige\", data=df ).fit()\n",
    "grad_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xs = np.linspace( 1, 10, 30 )\n",
    "ys = np.abs( xs - 5 ) < 1\n",
    "plt.scatter( xs, ys )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = pd.DataFrame(data = {\"x\" : xs, \"y\" : ys})\n",
    "bad['y'] = bad['y'].map( {False: 0, True: 1})\n",
    "bad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badmodel = sm.logit(data=bad, formula = \"y ~ x\").fit()\n",
    "badmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad.plot.scatter(x='x',y='y')\n",
    "\n",
    "x = np.linspace(bad['x'].min(),bad['x'].max(),1000)\n",
    "par = dict(badmodel.params)\n",
    "plt.plot(x,logistic(par['Intercept'] + par['x']*x),color='Black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(1, 10, 20)\n",
    "ys = np.abs(xs > 5) \n",
    "plt.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = pd.DataFrame(data = {\"x\" : xs, \"y\" : ys})\n",
    "good['y'] = good['y'].map( {False: 0, True: 1})\n",
    "good.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodmodel = sm.logit(data=good, formula = \"y ~ x\").fit()\n",
    "goodmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good.plot.scatter(x='x',y='y')\n",
    "\n",
    "x = np.linspace(good['x'].min(),good['x'].max(),1000)\n",
    "par = dict(goodmodel.params)\n",
    "plt.plot(x,logistic(par['Intercept'] + par['x']*x),color='Black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "xs = uniform.rvs(loc = -2, scale=4, size=1000)\n",
    "ys = uniform.rvs(loc = -2, scale=4, size=1000)\n",
    "classLabel = (xs*xs + ys*ys) < 1.5**2\n",
    "\n",
    "cf = pd.DataFrame({'x': xs, 'y': ys, 'label':classLabel})\n",
    "cf['label'] = cf['label'].map({False: 0, True: 1})\n",
    "cf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cf[cf['label'] == 1].plot.scatter('x', 'y', c='blue')\n",
    "cf[cf['label']== 0].plot.scatter('x', 'y', c='red', ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle_model = sm.logit(\"label ~ x + y\", data=cf).fit()\n",
    "circle_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cf[cf['label'] == 1].plot.scatter('x', 'y', c='blue')\n",
    "cf[cf['label']== 0].plot.scatter('x', 'y', c='red', ax=ax)\n",
    "\n",
    "# 0.5 = logistic(int + beta0*x + beta1*y)\n",
    "# logit(.5) = int + beta0*x + beta1*y\n",
    "# y = (logit .5 - int - beta0*x)/(beta 1)\n",
    "\n",
    "\n",
    "\n",
    "x = np.linspace(-2,2,100)\n",
    "par = dict(circle_model.params)\n",
    "plt.plot(x,(1/par['y'])*(logit(.5) - par['Intercept'] - par['x']*x),color='Black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "cf['dist'] = cf['x']*cf['x'] + cf['y']*cf['y'] + (norm.rvs(size=len(cf))/4)\n",
    "cf['dist'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_model = sm.logit(\"label ~ dist\", data=cf).fit()\n",
    "distance_model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "8fcd46d531329f49b73a0948faa8aed429eb8dafd35203d9948e8358a307ba0e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
