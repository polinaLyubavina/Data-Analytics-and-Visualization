{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical Clustering\n",
    "Adapted from *COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll continue our discussion of clustering, covering\n",
    "* agglomerative clustering \n",
    "* dendogram plots\n",
    "* distances\n",
    "* comparison of clustering methods on MNIST digits\n",
    "\n",
    "Recommended Reading:\n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 10.1 and 10.3. [digitial version available here](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "* J. Grus, Data Science from Scratch, Ch. 19\n",
    "* [scikit-learn documentation on clustering](http://scikit-learn.org/stable/modules/clustering.html)\n",
    "* [Jörn's SciPy Hierarchical Clustering and Dendrogram Tutorial](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Supervised vs. Unsupervised Learning\n",
    "\n",
    "### Supervised Learning\n",
    "**Data:** both the features, $x$, and a response, $y$, for each item in the dataset.\n",
    "\n",
    "**Goal:** 'learn' how to predict the response from the features.\n",
    "\n",
    "**Examples:**\n",
    "* Regression\n",
    "* Classification\n",
    "\n",
    "\n",
    "### Unsupervised Learning \n",
    "**Data:** Only the features, $x$, for each item in the dataset.\n",
    "\n",
    "**Goal:** discover 'interesting' things about the dataset.\n",
    "\n",
    "**Examples:**\n",
    "* Clustering\n",
    "* Dimensionality reduction, Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is the task of discovering unknown subgroups in data, which we call *clusters*.  In other words, the **goal** is to partition the datset into clusters where ‘similar’ items are in the same cluster and ‘dissimilar’ items are in different clusters. \n",
    "\n",
    "**Examples:**\n",
    "* Social Network Analysis: Clustering can be used to find communities\n",
    "* Ecology: cluster organisms that share attributes into species, genus, etc...\n",
    "* Handwritten digits where the digits are unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The k-means clustering method\n",
    "\n",
    "**Data:**  A collection of points $\\{x_i\\}$, for $i = 1,\\ldots n$, where $x_i\\in \\mathbb R^d$. \n",
    "\n",
    "In [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering), one tries to find $k$ *centers*, $\\{\\mu_\\ell\\}$, $\\ell = 1,\\ldots k$, and assign each point $x$ to a *cluster* $C_\\ell$ with center $\\mu_\\ell$, as to minimize the *total intra-cluster distance* \n",
    "$$\n",
    "\\arg\\min_{\\mu, C} \\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} \\| x_i - \\mu_\\ell\\|^2. \n",
    "$$\n",
    "Here, $\\mu_\\ell$ is the mean of points in $C_\\ell$. The total intra-cluster distance is the total squared Euclidean distance from each point to the center of its cluster. It's a measure of the varaince or internal coherence of the clusters. \n",
    "\n",
    "### Lloyd's Algorithm\n",
    "\n",
    "\n",
    "**Input:** set of points $x_1,\\ldots, x_n$ and an integer $k$ (# clusters)\n",
    "\n",
    "Pick $k$ starting points as centers $\\mu_1, \\ldots, \\mu_k$.\n",
    "\n",
    "** while** not converged:\n",
    "1. Assign each point $x_i$, to the cluster $C_\\ell$ with closest center $\\mu_\\ell$. \n",
    "2. For each cluster $C_\\ell$, compute a new center, $\\mu_\\ell$, by taking the mean of all $x_i$ assigned to cluster $C_\\ell$, *i.e.*, \n",
    "$$\n",
    "\\mu_\\ell = \\frac{1}{|C_\\ell|}\\sum_{x_i \\in C_\\ell} x_i\n",
    "$$\n",
    "\n",
    "\n",
    "### Performance and properties of k-means\n",
    "\n",
    "* The run time is $O(n*k*d*i)$ where \n",
    " - n is the number of items,\n",
    " - k is the number of clusters\n",
    " - d is the number of dimensions of the feature vectors\n",
    " - i is the number of iterations needed until convergence. \n",
    " \n",
    "  For data that has well-defined clusters, $i$ is typically small. In practice, the $k$-means algorithm is very fast. \n",
    "\n",
    "* Lloyds algorithm finds a *local optimum*, not necessarily the *global optimum*\n",
    "\n",
    "  Since the algorithm is fast, it is common to run the algorithm multiple times and pick the solution with the smallest total intra-cluster distance, \n",
    "$$\n",
    "\\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} \\| x_i - \\mu_\\ell\\|^2. \n",
    "$$\n",
    "\n",
    "* The total intra-cluster distance doesn't increase at every iteration of Lloyd's algorithm\n",
    "\n",
    "* The total intra-cluster distance decreases with larger $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cluster evaluation \n",
    "\n",
    "### Evaluating clusters without ground-truth labels\n",
    "\n",
    "1. Visual comparison\n",
    "+ Use the total intra-cluster distance (useful for k-means)\n",
    "+ [Silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "\n",
    "\n",
    "### Evaluating cluster quality with known ground-truth labels\n",
    "\n",
    "1. [`homogeneity_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html): Homogeneity metric of a cluster labeling given a ground truth. A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "+ [`completeness_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html): A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "+ [`v_measure_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html): The V-measure is the harmonic mean between homogeneity and completeness:\n",
    "$$\n",
    "v = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "$$\n",
    "+ [`homogeneity_completeness_v_measure`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html): Compute the homogeneity, completeness, and v-Measure scores at once.\n",
    "+ Confusion matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "[Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) is a collection of methods for clustering, where we don't just find a single clustering of the data, but a hierarchy of clusters. There are two main strategies for hierarchical clustering:\n",
    "\n",
    "1. **Agglomerative:** This \"bottom up\" approach to clustering starts with each data point in its own cluster. Pairs of clusters are merged as one moves up the hierarchy.\n",
    "\n",
    "+ **Divisive:** This \"top down\" approach to clustering starts with all observations in one cluster. Splits of the clusters are made recursively as one moves down the hierarchy.\n",
    "\n",
    "We'll focus on Agglomerative Clustering and use the `AgglomerativeClustering` function in  scikit-learn.\n",
    "\n",
    "\n",
    "### Agglomerative clustering\n",
    "* Start with each item as it’s own cluster.\n",
    "+ Link together the two clusters that are 'closest together' and store this information in the dendrogram plot. \n",
    "+ Continue this process until there is only one cluster.\n",
    "+ Using the dendrogram plot, decide which clustering is best.\n",
    "\n",
    "<img src=\"dendrogram.png\" width=\"500\">\n",
    "\n",
    "### Linkage methods in Agglomerative clustering:\n",
    "* **Maximum or complete linkage**: the maximum distance between observations of pairs of clusters, \n",
    "$$\n",
    "\\max\\{d(a,b)\\colon a \\in A, b \\in B \\}.\n",
    "$$\n",
    "\n",
    "* **Minimum linkage**: the minimum distance between observations of pairs of clusters, \n",
    "$$\n",
    "\\min\\{d(a,b)\\colon a \\in A, b \\in B \\}.\n",
    "$$\n",
    "\n",
    "* **Average linkage**: the average of the distances between all observations of pairs of clusters,\n",
    "$$\n",
    "\\frac{1}{|A| |B|} \\sum_{a \\in A, b \\in B} d(a, b).\n",
    "$$\n",
    "\n",
    "* **Centroid distance**:  if $c_A$ and $c_B$ are the centers of clusters $A$ and $B$, then $d(c_A,c_B)$.\n",
    "\n",
    "* **Ward** minimizes the total inner-cluster distance, similiar to as in $k$-means.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "#import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris, load_digits\n",
    "from sklearn.cluster import *\n",
    "# from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics import homogeneity_score, homogeneity_completeness_v_measure\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# Create color maps\n",
    "cmap = ListedColormap([\"Red\",\"Green\",\"Blue\"])\n",
    "# cmap = ListedColormap([\"#e41a1c\",\"#984ea3\",\"#a65628\",\"#377eb8\",\"#ffff33\",\"#4daf4a\",\"#ff7f00\"])\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: blob dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=3, random_state=1)\n",
    "\n",
    "# if we know there are 3 clusters\n",
    "agg_cluster_model = AgglomerativeClustering(linkage='average', affinity='euclidean', n_clusters=3)\n",
    "y_pred = agg_cluster_model.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a dendogram plot\n",
    "\n",
    "A [dendogram plot](https://en.wikipedia.org/wiki/Dendrogram) can be used to decide the number, $k$, of clusters.\n",
    "\n",
    "The `scipy.cluster.hierarchy` function [`linkage`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage) returns an array of length $n-1$ that contains all cluster merging information. Each row has the format \n",
    "`[idx1, idx2, dist, sample_count]`.\n",
    "\n",
    "The `scipy.cluster.hierarchy` function [`fcluster`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.fcluster.html) can then be used to extract the clusters from the linkage array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(X, 'ward') # generate the linkage array\n",
    "print(Z[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dendrogram plot\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=10.,  # font size for the x axis labels\n",
    "    color_threshold= 5 # changes where we go from blue to \"cluster colors\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fcluster(Z=Z, t=3, criterion='maxclust')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: two moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_moons, y = make_moons(n_samples=500, noise=.05)\n",
    "\n",
    "agg_cluster_model = AgglomerativeClustering(linkage=\"complete\", affinity='euclidean', n_clusters=2)\n",
    "y_pred = agg_cluster_model.fit_predict(X_moons)\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Adding connectivity constraints\n",
    "\n",
    "Previously, we joined clusters based soley on distance. Here we introduce a [connectivity constraint](http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py) based on k-Nearest Neighbors graph so that only adjacent clusters can be merged together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "connectivity = kneighbors_graph(X_moons, n_neighbors=10, include_self=False)\n",
    "\n",
    "agg_cluster_model = AgglomerativeClustering(linkage=\"complete\", connectivity=connectivity, n_clusters=2,compute_full_tree=True)\n",
    "y_pred = agg_cluster_model.fit_predict(X_moons)\n",
    "\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Iris Dataset\n",
    "\n",
    "Recall the Iris dataset consists of 4 measurements for 150 different examples of irises. We know that there are $k=3$  species of irises in the dataset. Without using the labels, let's try to find them. Note: this is a harder problem than classification since we're not using the (known) labels! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset \n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "agg_cluster_model = AgglomerativeClustering(linkage=\"ward\", affinity='euclidean', n_clusters=3)\n",
    "y_pred = agg_cluster_model.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Evaluating the clusters\n",
    "\n",
    "As we saw last time, we can use the sklearn function [`homogeneity_completeness_v_measure`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html) to compute the homogeneity, completeness, and v-Measure scores at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "homogeneity_completeness_v_measure(labels_true = y, labels_pred = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "models = [AgglomerativeClustering(linkage=\"ward\", affinity='euclidean', n_clusters=3), \n",
    "         AgglomerativeClustering(linkage=\"average\", affinity='euclidean', n_clusters=3), \n",
    "         AgglomerativeClustering(linkage=\"complete\", affinity='euclidean', n_clusters=3), \n",
    "         AgglomerativeClustering(linkage=\"average\", affinity='manhattan', n_clusters=3),\n",
    "         AgglomerativeClustering(linkage=\"complete\", affinity='manhattan', n_clusters=3),\n",
    "         KMeans(n_clusters=3)]\n",
    "h = np.zeros([len(models),3])\n",
    "for i,m in enumerate(models):\n",
    "    y_pred = m.fit_predict(X)\n",
    "    h[i,:] = homogeneity_completeness_v_measure(labels_true = y, labels_pred = y_pred)\n",
    "    print(h[i,:])\n",
    "\n",
    "print('The winner is model #' + str(np.argmax(h[:,0]))\n",
    "      + ' with homogeneity score ' + str(np.max(h[:,0])))\n",
    "print('Method details:')\n",
    "print(models[np.argmax(h[:,0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measures of distance\n",
    "1. **Euclidean distance**:\n",
    "$$\n",
    "d(x,y) = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2 }\n",
    "$$\n",
    "+ **Manhattan distance**:\n",
    "$$\n",
    "d(x,y) = \\sum_{i=1}^d |x_i - y_i|\n",
    "$$\n",
    "+ **Correlation**: \n",
    "$$\\frac{\\sum ^n _{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum ^n _{i=1}(x_i - \\bar{x})^2} \\sqrt{\\sum ^n _{i=1}(y_i - \\bar{y})^2}}\n",
    "$$\n",
    "where $\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i$ and $\\bar{y}=\\frac{1}{n}\\sum_{i=1}^n y_i$ are the means.\n",
    "+ If $A$ and $B$ are two sets, we define the Jaccard similarity coefficient\n",
    "$$\n",
    "J(A,B) = \\frac{ |A\\cap B|}{ |A\\cup B| }.\n",
    "$$\n",
    "We always have that $0 \\leq J(A,B) \\leq 1$. We then define the **Jaccard  distance** as \n",
    "$$\n",
    "d(A,B) = 1 - J(A,B).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: MNIST dataset\n",
    "\n",
    "The MNIST handwritten digit dataset consists of images of handwritten digits, together with labels indicating which digit is in each image. \n",
    "\n",
    "Becaue both the features and the labels are present in this dataset (and labels for large datasets are generally difficult/expensive to obtain), this dataset is frequently used as a benchmark to compare various methods. \n",
    "For example, [this webpage](http://yann.lecun.com/exdb/mnist/) describes a variety of different classification results on MNIST (Note, the tests on this website are for a larger and higher resolution dataset than we'll use.) To see a comparison of classification methods implemented in scikit-learn on the MNIST dataset, see \n",
    "[this page](http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html).\n",
    "The MNIST dataset is also a frequently used for benchmarking clustering algorithms and because it has labels, we can evaluate the homogeneity or purity of the clusters. \n",
    "\n",
    "There are several versions of the dataset. We'll use the one that is built-in to scikit-learn, described [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html). \n",
    "\n",
    "* Classes: 10  \n",
    "* Samples per class: $\\approx$180\n",
    "* Samples total: 1797\n",
    "* Dimensionality: 64 (8 pixels by 8 pixels)\n",
    "* Features: integers 0-16\n",
    "\n",
    "Here are some examples of the images. Note that the digits have been size-normalized and centered in a fixed-size ($8\\times8$ pixels) image.\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_digits_classification_001.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT!!!\n",
    "\n",
    "Since this data set has labels, we would normally be performing **classification** not **clustering**.  We're using this data set here because we know there should be 10 good clusters.  Again, the cluster IDs are completely arbitrary and are not related to the labels at all.  We will completely ignore the labels except when we're computing homogeneity/completeness scores!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "print(type(X))\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "print(\"n_digits: %d, n_samples %d, n_features %d\" % (n_digits, n_samples, n_features))\n",
    "\n",
    "plt.figure(figsize= (10, 10))\n",
    "for ii in np.arange(25):\n",
    "    plt.subplot(5, 5, ii+1)\n",
    "    plt.imshow(np.reshape(X[ii,:],(8,8)), cmap='Greys',interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MNIST: k-means clustering\n",
    "We first use k-means method to cluster the dataset and compute the homogeneity score for the clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# fit k-means to data.  We're NOT using labels here!!!\n",
    "kmeans_model = KMeans(n_clusters=n_digits, n_init=10)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "\n",
    "# use labels to compute homogeneity score\n",
    "metrics.homogeneity_completeness_v_measure(labels_true=y, labels_pred=kmeans_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MNIST: Hierarchical clustering\n",
    "**Exercise:** Use a hierarchical clustering method to cluster the dataset. Again compute the homogeneity. \n",
    "\n",
    "*Hint:* Use the scikit-learn function *AgglomerativeClustering*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "agModel = AgglomerativeClustering(linkage=\"ward\", affinity='euclidean', n_clusters=n_digits)\n",
    "agModel.fit(X)\n",
    "\n",
    "metrics.homogeneity_completeness_v_measure(labels_true=y, labels_pred=agModel.labels_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MNIST: Other clustering methods\n",
    "Take a look at the clustering methods and options for various methods on the [scikit-learn page](http://scikit-learn.org/stable/modules/clustering.html). \n",
    "\n",
    "**Exercise:** By modifying the following code, try to find the clustering method with the largest homogeneity score for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(50 * '_')\n",
    "print('% 9s' % 'method' + '                   time' + '      homogeneity')\n",
    "def compare_method(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('% 25s   %.2fs     %.3f ' % (name, (time() - t0), metrics.homogeneity_score(y, estimator.labels_)))\n",
    "\n",
    "method = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "compare_method(estimator=method, name=\"k-means++\", data=X)\n",
    "\n",
    "method = KMeans(init='random', n_clusters=n_digits, n_init=10)\n",
    "compare_method(estimator=method, name=\"random\", data=X)\n",
    "\n",
    "method = AgglomerativeClustering(linkage=\"ward\", affinity='euclidean', n_clusters=n_digits)\n",
    "compare_method(estimator=method, name=\"hierarchicalWard\", data=X)\n",
    "\n",
    "method = AgglomerativeClustering(linkage=\"average\", affinity='manhattan', n_clusters=n_digits)\n",
    "compare_method(estimator=method, name=\"averageManhattan\", data=X)\n",
    "\n",
    "method = AgglomerativeClustering(linkage=\"complete\", affinity='manhattan', n_clusters=n_digits)\n",
    "compare_method(estimator=method, name=\"completeManhattan\", data=X)\n",
    "\n",
    "method = AgglomerativeClustering(linkage=\"complete\", affinity='euclidean', n_clusters=n_digits)\n",
    "compare_method(estimator=method, name=\"completeEuclidean\", data=X)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fcd46d531329f49b73a0948faa8aed429eb8dafd35203d9948e8358a307ba0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
